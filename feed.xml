<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://profile.bpanigrani.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://profile.bpanigrani.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-26T23:27:01+00:00</updated><id>https://profile.bpanigrani.com/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Getting Certified as TensorFlow Developer by Google</title><link href="https://profile.bpanigrani.com/blog/2023/getting-certified-as-tensorflow-developer-by-google/" rel="alternate" type="text/html" title="Getting Certified as TensorFlow Developer by Google"/><published>2023-07-08T21:53:45+00:00</published><updated>2023-07-08T21:53:45+00:00</updated><id>https://profile.bpanigrani.com/blog/2023/getting-certified-as-tensorflow-developer-by-google</id><content type="html" xml:base="https://profile.bpanigrani.com/blog/2023/getting-certified-as-tensorflow-developer-by-google/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Graph Coloring by Example</title><link href="https://profile.bpanigrani.com/blog/2022/graph-coloring-by-example/" rel="alternate" type="text/html" title="Graph Coloring by Example"/><published>2022-12-29T05:53:04+00:00</published><updated>2022-12-29T05:53:04+00:00</updated><id>https://profile.bpanigrani.com/blog/2022/graph-coloring-by-example</id><content type="html" xml:base="https://profile.bpanigrani.com/blog/2022/graph-coloring-by-example/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Drug Discovery Analysis</title><link href="https://profile.bpanigrani.com/blog/2022/images/" rel="alternate" type="text/html" title="Drug Discovery Analysis"/><published>2022-10-29T21:01:00+00:00</published><updated>2022-10-29T21:01:00+00:00</updated><id>https://profile.bpanigrani.com/blog/2022/images</id><content type="html" xml:base="https://profile.bpanigrani.com/blog/2022/images/"><![CDATA[<p>The code for this project could be found <a href="https://github.com/bhaba-ranjan/drug-discovery-ANN">here</a>. The following is an <code class="language-plaintext highlighter-rouge">analysis</code> of what I obsorved along the way.</p> <h2 class="mb-3">Neural Network</h2> <p>Neural networks worked promisingly in this task and outperformed accuracy of decision trees by <code class="language-plaintext highlighter-rouge">6%</code>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/corss-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/corss-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/corss-1400.webp"/> <img src="/assets/img/corss.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/loss-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/loss-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/loss-1400.webp"/> <img src="/assets/img/loss.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Cross validation F1-score (Left) and Loss (Right) </div> <h5 class="mb-3">Regularization</h5> <p>The majority of effort went <code class="language-plaintext highlighter-rouge">into choosing the best size neural network</code> with respective <code class="language-plaintext highlighter-rouge">regularization</code> hypermeter. As soon as I started training <strong>without regularization</strong> the NN started <strong>overfitting</strong> to the data starting from <em>2nd iteration</em>. After some research it was apparent to use regularization parameters. I had 3 options: <code class="language-plaintext highlighter-rouge">L1, L2 regularization and Dropout</code> method. L1 and L2 penalizes weights while drop out omits certain neurons. <code class="language-plaintext highlighter-rouge">L1 could make a weight 0</code> while <code class="language-plaintext highlighter-rouge">L2 is extremely slow</code> in pushing weight <code class="language-plaintext highlighter-rouge">towards 0</code>. I did not use the feature subset. In other words, I wanted the <code class="language-plaintext highlighter-rouge">NN to decide which feature to omit/reduce</code> by using loss fucntion and backpropagation. I wanted the weights to be 0 for irrelevant features so, I used <code class="language-plaintext highlighter-rouge">L1 regularization</code>.</p> <p>Choosing the right value for L1 regularization was another challenging task. To come up with a value that works, I tried different ùù∫ values. Went with <code class="language-plaintext highlighter-rouge">bigger values first</code>. They did <strong>not let the model fit</strong> to the data. Then I tried smaller values which <strong>led to overfitting</strong>. Sort of like <strong>binary search</strong> I used intermediate values to come up with the value that worked.</p> <h5 class="mb-3">Loss Function</h5> <p>Initially, I used <code class="language-plaintext highlighter-rouge">Binary Cross Entropy</code> as the loss function but the issue was it was not able to produce good results. <code class="language-plaintext highlighter-rouge">As it does not take imbalance into account</code>. Also, gradient descent updates the weight for prediction towards the confident class. Because, the majority class dominates the loss function. <code class="language-plaintext highlighter-rouge">Binary Focal Cross Entropy</code> however, was a better choice because it down weights easy class prediction and forces the model to learn parameters towards the hard to classify class. Which, is what we need as the data is imbalanced.</p> <h2 class="mb-3">Decision Trees</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/tree1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/tree1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/tree1-1400.webp"/> <img src="/assets/img/tree1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/tree2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/tree2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/tree2-1400.webp"/> <img src="/assets/img/tree2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As the data is less, the decision tree started to <strong>overfit on the training</strong> data as soon as I started training it. Which performed very badly on the testing data set. To avoid overfitting I tried 2 different methods. <code class="language-plaintext highlighter-rouge">Pre-pruning and Post-pruning</code>. Pre-pruning however, was not able to produce good results. As it is <strong>hard to understand</strong> optimal <strong>depth</strong> of the tree and <strong>minimum records</strong> to split at any level by just looking at the data.</p> <p>I found <strong>Post-pruning</strong> promising as it prunes the tree and then determines the accuracy and further prunes it. I was able to get a boost of 5% in the F1-Score after using post pruning in miner2. I used cost-complexity-pruning method. In this method we assign a score to every tree. The score is defined as <code class="language-plaintext highlighter-rouge">Tree Score = Misclassification + …ë * (Number of terminal nodes )</code>.</p> <p>The idea is to reduce the number of leaves and miss classification error while increasing the alpha. Finally the tree alpha where the tree has higher training and testing accuracy will be chosen.</p>]]></content><author><name></name></author><category term="Project-Analysis"/><summary type="html"><![CDATA[this is what included images could look like]]></summary></entry></feed>